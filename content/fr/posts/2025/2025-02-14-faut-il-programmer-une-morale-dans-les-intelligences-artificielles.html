---
title: >-
  Faut-il programmer une morale dans les intelligences artificielles ?
subtitle: >-
  
bodyclass: >-
   
url: "/productions/2025-02-14-faut-il-programmer-une-morale-dans-les-intelligences-artificielles/"
slug: "faut-il-programmer-une-morale-dans-les-intelligences-artificielles"
date: 2025-02-14T00:00:00+01:00
lastmod: 2025-02-21T14:42:50+01:00
meta:
  hugo:
    permalink: "/productions/2025-02-14-faut-il-programmer-une-morale-dans-les-intelligences-artificielles/"
    path: "/posts/2025/2025-02-14-faut-il-programmer-une-morale-dans-les-intelligences-artificielles"
    file: "content/fr/posts/2025/2025-02-14-faut-il-programmer-une-morale-dans-les-intelligences-artificielles.html"
    slug: "faut-il-programmer-une-morale-dans-les-intelligences-artificielles"
  dates:
    created_at: 2025-02-19T15:23:21+01:00
    updated_at: 2025-02-21T14:42:50+01:00
    published_at: 2025-02-14T00:00:00+01:00
search:
  id: "af46ab0a-4b05-4314-a5b4-b0eeac9e60ec"
  url: "/productions/2025-02-14-faut-il-programmer-une-morale-dans-les-intelligences-artificielles/"
  kind: "Communication::Website::Post::Localization"
  lang: "fr"
  title: >-
    Faut-il programmer une morale dans les intelligences artificielles ?
  summary: >-
    <p>Dans cet épisode de "Controverse", nous explorons une question fascinante et controversée : faut-il programmer une morale dans les intelligences artificielles ? </p>
  body: >-
    <p>  <br>Cette interrogation soulève des enjeux éthiques, scientifiques et sociétaux majeurs, allant des biais algorithmiques aux dilemmes moraux des voitures autonomes.</p>  <p>   <br>Pour offrir une analyse approfondie et dynamique, nous avons adopté une approche mêlant : <br>Des explications scientifiques : Nous nous appuyons sur des travaux de recherche et des concepts philosophiques pour définir la moralité appliquée à l'IA. <br>Des extraits de micro-trottoir : Nous avons recueilli des avis variés de personnes issues de différents horizons (MMI, comptabilité, cybersécurité) pour enrichir la discussion. <br>Une approche interactive : Le podcast alterne entre discussions entre animateurs et interventions du public pour dynamiser le débat.</p>  <p>   <br>Bonjour à toutes et tous et bienvenue dans le podcast Controverse. Aujourd'hui, on s'attaque à une question brûlante. L'intelligence artificielle doit-elle avoir une moralité programmée ? Une question qui divise autant les experts que le grand public. Pour y répondre, nous avons recueilli des témoignages d'étudiants en MMI, en comptabilité et en cybersécurité, qu'on a ensuite croisée avec des explications scientifiques. Pour commencer, posons les bases. Une IA est-elle capable d'avoir une morale ? Selon le philosophe Martin Gibert, Chat GPT, par exemple, n'est pas un patient moral, c'est-à-dire qu'il ne peut pas être blessé, mais il pourrait être un agent moral artificiel, car il est conçu pour donner des réponses conformes aux normes humaines. Cette moralité repose sur trois couches, les données d'entraînement, le feedback humain pour affiner le modèle et des filtres qui empêchent certaines réponses. Mais alors, est-ce qu'on peut vraiment dire qu'une IA a une morale ? Vu comme on l'alimente avec ce qu'on pense, elle peut potentiellement avoir des biais, répondre en fonction du sujet qu'on a. Aujourd'hui, l'intelligence artificielle, ça régit genre notre vie parce qu'on en fait déjà. On peut imaginer, oui, il y a des intelligences artificielles qui sont utilisées pour la guerre, etc. Déjà des drones russes, sont utilisés, et tout et elles n'ont aucune notion du bien et du mal. C'est pour ça qu'en fait pour moi, c'est important de réussir à intégrer une sorte de conscience ou une sorte de notion du bien et du mal. Ce qui nous amène à un vrai dilemme.Peut-on programmer une morale dans une IA ? Et surtout, qui devrait décider de cette morale ? J’ai envie de dire peut être des services plus que j’ai envie de dire comme de l'ONU ou des choses comme ça. Parce que malheureusement, les entreprises types Google ou Chat GPT, enfin Chat Open AI et tout ça, sont là aussi pour l'aspect business, donc c'est compliqué. Ils pourraient instaurer une morale, mais qui leur profiterait derrière, donc on ne serait pas sûr de l’aspect transparent et éthique de la démarche. Je dirais, je sais pas si c'est ça pourrait être quelque chose du genre, on va un référendum avec du coup le peuple qui décide ou alors les grands dirigeants, même si on sait que les grands dirigeants ne sont pas forcément de très bonnes personnes et que des fois il y en a qui sont poursuivis pour des choses qui sont considérées mauvaises en tout cas comme étant mauvaises et qui je pense sont mauvaises. Donc c'est vrai que c'est donner la parole aux dirigeants, c'est un peu compliqué, mais en même temps, donner la parole au peuple, ça peut donner des choses pareilles très différentes. Donc peut-être les scientifiques se battraient sur les lois de base des lois actuelles entre ce qui est bon ou ce qui n'est pas bon.En effet, les entreprises qui développent ses IA ont aussi leurs propres intérêts économiques et politiques. C'est ce qu'on appelle les barricades morales, des cadres éthiques censés guider l'IA, mais qui sont souvent influencées par des intérêts privés. Et si une IA fonctionne sans aucun cadre moral, quels seraient les risques ? Je pourrais demander de m'aider à créer une bombe et faire un attentat. Donc c'est pas bien. Une IA. Si tu ne la cadres pas, elle fait ce qu'elle veut. En fait, elle va définir une marche à suivre qui pourra est la bonne. Mais c'est pas ce qu'on a demandé d'elle etc. Et à l'inverse, une IA trop morale, c'est un problème aussi. Par exemple, si une voiture autonome doit choisir entre protéger ses passagers ou minimiser le nombre de victimes, que doit-elle faire ? Ça pourrait encore une fois revenir, par exemple, au constructeur de la voiture qui dit sa marque, oui, dans tous les cas, vous serez sécurisé, ou bien elle que du coup elle. Dans tous les cas, elle protégera les personnes dans la voiture.  Alors de manière à ce que ce soit vendeur, je pense qu'il faut que ça sécurisant enfin de sécuriser ses propres passagers. Mais je pense que le mieux c'est quand même que de minimiser le nombre de personnes blessées par l'accident. Parce que et bien en soit, c'est ça aussi le truc intéressant, enfin intéressant dans l’IA. C’est que bah elle prend des décisions sans du tout avoir d'émotion ou quoi que ce soit, et juste l'aspect pratique qui est. C'est juste on lui demande une chose et elle va l'appliquer, peu importe les conséquences en soi. Finalement, la question de la moralité des IA reflète nos propres dilemmes humains. Peut-être que, avant d'enseigner une morale aux machines, on devrait déjà s'accorder sur la nôtre. Avant de conclure, on vous a posé une dernière question Si vous avez une IA ultra-puissante à votre service, quelle règle morale lui imposeriez-vous ? Je sais pas, il y en a trop. Peut-être l'idée de transparence justement. Par exemple, quand l'IA n'a pas forcément une information qu'on ne cherche pas, on donne une fausse. Rester plutôt sur quelque chose en disant, je n'ai pas accès à cette information. Je ne sais pas. L'unique règle que je enfin si je devais en choisir une, c'est ne fait rien sans mon accord. C'est en fait, ça serait presque l'empêcher de faire l'empêcher d'utiliser sa super intelligence entre guillemets. Mais au moins on garde le contrôle. Aujourd'hui, il y a trop d'IA qui fonctionnent sans qu'on comprenne ce qu'elles font en fait, elles travaillent, on sait qu'elles travaillent, on sait, on connaît le résultat, mais on sait pas comment ils sont arrivés, etc. Et c'est, je crois, la seule règle que je mettrais si je devais vraiment choisir. </p>  <p>   <br>En savoir plus</p>  <p>https://audioblog.arteradio.com/blog/203278/podcast/244774/controverse-faut-il-programmer-une-morale-dans-les-intelligences-artificielles</p>  <p> Cartographie des controverses  <br> Bibliographie   <br>   <br>Beaudouin, V., & Velkovska, J. (2023). Enquêter sur l’« éthique de l’IA ». Réseaux, 240(4), 9‑27. https://doi.org/10.3917/res.240.0009Éthique et intelligence artificielle. (2020). Revue d’éthique et de théologie morale, N° 307(3), 146. Cairn.info. https://shs.cairn.info/revue-d-ethique-et-de-theologie-morale-2020-3?lang=frGibert, M. (2023, octobre 5). ChatGPT est-iel un agent moral artificiel ? - AOC media. AOC media - Analyse Opinion Critique. https://aoc.media/opinion/2023/10/05/chatgpt-est-iel-un-agent-moral-artificiel/Tassinari, C. A., De Martino, S., & Ferguson, Y. (2024). Moraliser les machines communicantes. Des barricades morales à l’éthique située : Trois cas d’usage de l’IA en milieu professionnel. Communiquer. Revue de communication sociale et publique, 39, Article 39. https://doi.org/10.4000/12zlnUNESCO. (s. d.). Éthique de l’intelligence artificielle | UNESCO. https://www.unesco.org/fr/artificial-intelligence/recommendation-ethics</p>

breadcrumbs:
  - title: >-
      Accueil
    path: "/"
  - title: >-
      Productions
    path: "/productions/"
  - title: >-
      Faut-il programmer une morale dans les intelligences artificielles ?

design:
  full_width: false
  toc:
    present: true
    offcanvas: false

authors:
  - "diogo-rios-minaya"
  - "julie-rondof"
  - "claire-vital"
translationKey: communication-website-post-26e10914-3673-42e0-8638-4d3284ba74a8

image:
  id: "23ed60cd-7806-45f5-aaff-224496f3e6ff"
  alt: ""
  credit: >-
    


meta_description: >-
  

summary: >-
  <p>Dans cet épisode de "Controverse", nous explorons une question fascinante et controversée : faut-il programmer une morale dans les intelligences artificielles ? </p>

header_cta:
  display: false
  label: >-
    
  target: ""
  external: false
posts_categories:
  - "theme/ia"
  - "controverse/lia-doit-elle-avoir-une-moralite-programmee"
  - "format/podcast"
taxonomies:
  - name: "Thème"
    slug: "theme"
    path: "/posts_categories/theme"
    categories:
      - permalink: "/productions/theme/ia/"
        path: "/posts_categories/theme/ia"
        slug: "theme/ia"
        file: "content/fr/posts_categories/theme/ia/_index.html"
        name: "IA"

  - name: "Format"
    slug: "format"
    path: "/posts_categories/format"
    categories:
      - permalink: "/productions/format/podcast/"
        path: "/posts_categories/format/podcast"
        slug: "format/podcast"
        file: "content/fr/posts_categories/format/podcast/_index.html"
        name: "Podcast"

  - name: "Controverse"
    slug: "controverse"
    path: "/posts_categories/controverse"
    categories:
      - permalink: "/productions/controverse/lia-doit-elle-avoir-une-moralite-programmee/"
        path: "/posts_categories/controverse/lia-doit-elle-avoir-une-moralite-programmee"
        slug: "controverse/lia-doit-elle-avoir-une-moralite-programmee"
        file: "content/fr/posts_categories/controverse/lia-doit-elle-avoir-une-moralite-programmee/_index.html"
        name: "L'IA doit-elle avoir une moralité programmée ?"


contents_reading_time:
  seconds: 340
  text: >-
    6 minutes
contents:
  - kind: block
    template: chapter
    title: >-
      
    slug: >-
      
    ranks:
      base: 2
    top:
      active: false
    data:
      layout: no_background
      text: >-
        <p>Cette interrogation soulève des enjeux éthiques, scientifiques et sociétaux majeurs, allant des biais algorithmiques aux dilemmes moraux des voitures autonomes.</p>

      notes: >-
        


      alt: >-
        

      credit: >-
        



  - kind: block
    template: chapter
    title: >-
      
    slug: >-
      
    ranks:
      base: 2
    top:
      active: false
    data:
      layout: no_background
      text: >-
        <p><br> Pour offrir une analyse approfondie et dynamique, nous avons adopté une approche mêlant : Des explications scientifiques : Nous nous appuyons sur des travaux de recherche et des concepts philosophiques pour définir la moralité appliquée à l'IA. Des extraits de micro-trottoir : Nous avons recueilli des avis variés de personnes issues de différents horizons (MMI, comptabilité, cybersécurité) pour enrichir la discussion. Une approche interactive : Le podcast alterne entre discussions entre animateurs et interventions du public pour dynamiser le débat.<br> </p>

      notes: >-
        


      alt: >-
        

      credit: >-
        



  - kind: block
    template: embed
    title: >-
      
    slug: >-
      
    ranks:
      base: 2
    top:
      active: false
    data:
      code: >-
        <iframe width="100%" src="https://audioblog.arteradio.com/embed/244774" style="margin: 0; padding: 0; border: none;" loading="lazy"></iframe>

      transcription_title: >-
        

      transcription: >-
        <p>Bonjour à toutes et tous et bienvenue dans le podcast Controverse. Aujourd'hui, on s'attaque à une question brûlante. L'intelligence artificielle doit-elle avoir une moralité programmée ? Une question qui divise autant les experts que le grand public. Pour y répondre, nous avons recueilli des témoignages d'étudiants en MMI, en comptabilité et en cybersécurité, qu'on a ensuite croisée avec des explications scientifiques. </p><p>Pour commencer, posons les bases. Une IA est-elle capable d'avoir une morale ? Selon le philosophe Martin Gibert, Chat GPT, par exemple, n'est pas un patient moral, c'est-à-dire qu'il ne peut pas être blessé, mais il pourrait être un agent moral artificiel, car il est conçu pour donner des réponses conformes aux normes humaines. Cette moralité repose sur trois couches, les données d'entraînement, le feedback humain pour affiner le modèle et des filtres qui empêchent certaines réponses. Mais alors, est-ce qu'on peut vraiment dire qu'une IA a une morale ? </p><p>Vu comme on l'alimente avec ce qu'on pense, elle peut potentiellement avoir des biais, répondre en fonction du sujet qu'on a. Aujourd'hui, l'intelligence artificielle, ça régit genre notre vie parce qu'on en fait déjà. On peut imaginer, oui, il y a des intelligences artificielles qui sont utilisées pour la guerre, etc. Déjà des drones russes, sont utilisés, et tout et elles n'ont aucune notion du bien et du mal. C'est pour ça qu'en fait pour moi, c'est important de réussir à intégrer une sorte de conscience ou une sorte de notion du bien et du mal. Ce qui nous amène à un vrai dilemme.</p><p>Peut-on programmer une morale dans une IA ? Et surtout, qui devrait décider de cette morale ? </p><p>J’ai envie de dire peut être des services plus que j’ai envie de dire comme de l'ONU ou des choses comme ça. Parce que malheureusement, les entreprises types Google ou Chat GPT, enfin Chat Open AI et tout ça, sont là aussi pour l'aspect business, donc c'est compliqué. Ils pourraient instaurer une morale, mais qui leur profiterait derrière, donc on ne serait pas sûr de l’aspect transparent et éthique de la démarche. Je dirais, je sais pas si c'est ça pourrait être quelque chose du genre, on va un référendum avec du coup le peuple qui décide ou alors les grands dirigeants, même si on sait que les grands dirigeants ne sont pas forcément de très bonnes personnes et que des fois il y en a qui sont poursuivis pour des choses qui sont considérées mauvaises en tout cas comme étant mauvaises et qui je pense sont mauvaises. Donc c'est vrai que c'est donner la parole aux dirigeants, c'est un peu compliqué, mais en même temps, donner la parole au peuple, ça peut donner des choses pareilles très différentes. Donc peut-être les scientifiques se battraient sur les lois de base des lois actuelles entre ce qui est bon ou ce qui n'est pas bon.</p><p>En effet, les entreprises qui développent ses IA ont aussi leurs propres intérêts économiques et politiques. C'est ce qu'on appelle les barricades morales, des cadres éthiques censés guider l'IA, mais qui sont souvent influencées par des intérêts privés. Et si une IA fonctionne sans aucun cadre moral, quels seraient les risques ? </p><p>Je pourrais demander de m'aider à créer une bombe et faire un attentat. Donc c'est pas bien. Une IA. Si tu ne la cadres pas, elle fait ce qu'elle veut. En fait, elle va définir une marche à suivre qui pourra est la bonne. Mais c'est pas ce qu'on a demandé d'elle etc. </p><p>Et à l'inverse, une IA trop morale, c'est un problème aussi. Par exemple, si une voiture autonome doit choisir entre protéger ses passagers ou minimiser le nombre de victimes, que doit-elle faire ? </p><p>Ça pourrait encore une fois revenir, par exemple, au constructeur de la voiture qui dit sa marque, oui, dans tous les cas, vous serez sécurisé, ou bien elle que du coup elle. Dans tous les cas, elle protégera les personnes dans la voiture.  Alors de manière à ce que ce soit vendeur, je pense qu'il faut que ça sécurisant enfin de sécuriser ses propres passagers. Mais je pense que le mieux c'est quand même que de minimiser le nombre de personnes blessées par l'accident. Parce que et bien en soit, c'est ça aussi le truc intéressant, enfin intéressant dans l’IA. C’est que bah elle prend des décisions sans du tout avoir d'émotion ou quoi que ce soit, et juste l'aspect pratique qui est. C'est juste on lui demande une chose et elle va l'appliquer, peu importe les conséquences en soi. </p><p>Finalement, la question de la moralité des IA reflète nos propres dilemmes humains. Peut-être que, avant d'enseigner une morale aux machines, on devrait déjà s'accorder sur la nôtre. Avant de conclure, on vous a posé une dernière question Si vous avez une IA ultra-puissante à votre service, quelle règle morale lui imposeriez-vous ? </p><p>Je sais pas, il y en a trop. Peut-être l'idée de transparence justement. Par exemple, quand l'IA n'a pas forcément une information qu'on ne cherche pas, on donne une fausse. Rester plutôt sur quelque chose en disant, je n'ai pas accès à cette information. Je ne sais pas. L'unique règle que je enfin si je devais en choisir une, c'est ne fait rien sans mon accord. C'est en fait, ça serait presque l'empêcher de faire l'empêcher d'utiliser sa super intelligence entre guillemets. Mais au moins on garde le contrôle. Aujourd'hui, il y a trop d'IA qui fonctionnent sans qu'on comprenne ce qu'elles font en fait, elles travaillent, on sait qu'elles travaillent, on sait, on connaît le résultat, mais on sait pas comment ils sont arrivés, etc. Et c'est, je crois, la seule règle que je mettrais si je devais vraiment choisir. </p>



  - kind: block
    template: links
    title: >-
      
    slug: >-
      
    ranks:
      base: 2
    top:
      active: false
    data:
      description: >-
        

      links:
        - title: >-
            En savoir plus

          alt_title: >-
            

          description: >-
            

          url: >-
            https://audioblog.arteradio.com/blog/203278/podcast/244774/controverse-faut-il-programmer-une-morale-dans-les-intelligences-artificielles

          external: true



  - kind: block
    template: title
    title: >-
      Cartographie des controverses
    slug: >-
      cartographie-des-controverses
    ranks:
      base: 2
      self: 2
    top:
      active: true
      title: 
        value: >-
          Cartographie des controverses
        heading: 2
    data:
      layout: collapsed


  - kind: block
    template: chapter
    title: >-
      
    slug: >-
      
    ranks:
      base: 3
    top:
      active: false
    data:
      layout: no_background
      text: >-
        

      notes: >-
        

      image:
        id: "1b929da4-7d72-4e35-9d04-27bf7d235057"
        file: "1b929da4-7d72-4e35-9d04-27bf7d235057"

      alt: >-
        

      credit: >-
        



  - kind: block
    template: title
    title: >-
      Bibliographie
    slug: >-
      bibliographie
    ranks:
      base: 2
      self: 2
    top:
      active: true
      title: 
        value: >-
          Bibliographie
        heading: 2
    data:
      layout: collapsed


  - kind: block
    template: chapter
    title: >-
      
    slug: >-
      
    ranks:
      base: 3
    top:
      active: false
    data:
      layout: no_background
      text: >-
        <p>Beaudouin, V., & Velkovska, J. (2023). Enquêter sur l’« éthique de l’IA ». Réseaux, 240(4), 9‑27.<a href="https://doi.org/10.3917/res.240.0009"> https://doi.org/10.3917/res.240.0009</a></p><p>Éthique et intelligence artificielle. (2020). Revue d’éthique et de théologie morale, N° 307(3), 146. Cairn.info.<a href="https://shs.cairn.info/revue-d-ethique-et-de-theologie-morale-2020-3?lang=fr"> https://shs.cairn.info/revue-d-ethique-et-de-theologie-morale-2020-3?lang=fr</a></p><p>Gibert, M. (2023, octobre 5). ChatGPT est-iel un agent moral artificiel ? - AOC media. AOC media - Analyse Opinion Critique.<a href="https://aoc.media/opinion/2023/10/05/chatgpt-est-iel-un-agent-moral-artificiel/"> https://aoc.media/opinion/2023/10/05/chatgpt-est-iel-un-agent-moral-artificiel/</a></p><p>Tassinari, C. A., De Martino, S., & Ferguson, Y. (2024). Moraliser les machines communicantes. Des barricades morales à l’éthique située : Trois cas d’usage de l’IA en milieu professionnel. Communiquer. Revue de communication sociale et publique, 39, Article 39.<a href="https://doi.org/10.4000/12zln"> https://doi.org/10.4000/12zln</a></p><p>UNESCO. (s. d.). Éthique de l’intelligence artificielle | UNESCO.<a href="https://www.unesco.org/fr/artificial-intelligence/recommendation-ethics"> https://www.unesco.org/fr/artificial-intelligence/recommendation-ethics</a></p>

      notes: >-
        


      alt: >-
        

      credit: >-
        



  - kind: block
    template: embed
    title: >-
      
    slug: >-
      
    ranks:
      base: 3
    top:
      active: false
    data:
      code: >-
        

      transcription_title: >-
        

      transcription: >-
        




---
